{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Environment & GPU Sanity"
      ],
      "metadata": {
        "id": "XC5SZ4SJyf0E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ua9h7ohndLmb",
        "outputId": "37fa2e84-714c-4d67-ee7b-9a126aaff99f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jan 24 12:08:31 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unsloth_Task_C_torch_compile.ipynb**\n",
        "\n",
        "\n",
        "Imports + torch.compile config"
      ],
      "metadata": {
        "id": "tuXPaNQHyWv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U transformers datasets peft trl bitsandbytes accelerate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZjgeCKogWCw",
        "outputId": "e5d028ea-92bd-4290-d839-cbae5d010da5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.6)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.5.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.1)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.12/dist-packages (0.27.1)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (23.0.0)\n",
            "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.9.0+cu126)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Cell 2: Imports + torch.compile config\n",
        "# ===============================\n",
        "\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import logging\n",
        "\n",
        "# Core HF / training libs\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "# Safety checks\n",
        "assert torch.cuda.is_available(), \"CUDA not available\"\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "print(\"CUDA version:\", torch.version.cuda)\n",
        "\n",
        "# ---------------------------------\n",
        "# torch.compile configuration\n",
        "# ---------------------------------\n",
        "torch_compile_options = {\n",
        "    \"epilogue_fusion\"   : True,\n",
        "    \"max_autotune\"      : True,   # important for matmul tuning\n",
        "    \"shape_padding\"     : True,   # helps dynamic shapes\n",
        "    \"trace.enabled\"     : True,\n",
        "    \"triton.cudagraphs\" : False,  # avoid instability\n",
        "}\n",
        "\n",
        "# Global defaults\n",
        "torch.set_default_dtype(torch.float16)\n",
        "\n",
        "print(\"Cell 2 loaded successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwcE-wFNzN3_",
        "outputId": "30003872-dd62-42f3-af25-e1957ee4f52d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 2.9.0+cu126\n",
            "CUDA version: 12.6\n",
            "Cell 2 loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CELL 3 — torch.compile logging (graph breaks + recompiles)**"
      ],
      "metadata": {
        "id": "5S7dG5w0za8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Cell 3: torch.compile logging & diagnostics\n",
        "# ===============================\n",
        "\n",
        "# Environment variables for detailed logs\n",
        "os.environ[\"TORCHDYNAMO_VERBOSE\"] = \"1\"\n",
        "os.environ[\"TORCHINDUCTOR_FORCE_DISABLE_CACHES\"] = \"1\"\n",
        "os.environ[\"TORCHINDUCTOR_COMPILE_THREADS\"] = \"1\"\n",
        "\n",
        "# Torch internal debug flags\n",
        "torch._inductor.config.debug = True\n",
        "torch._dynamo.config.verbose = True\n",
        "torch._dynamo.config.suppress_errors = False  # DO NOT suppress errors\n",
        "\n",
        "# Enable detailed logging\n",
        "torch._logging.set_logs(\n",
        "    dynamo = logging.WARN,\n",
        "    inductor = logging.WARN,\n",
        "    graph_breaks = True,\n",
        "    recompiles = True,\n",
        "    recompiles_verbose = True,\n",
        "    compiled_autograd_verbose = True,\n",
        ")\n",
        "\n",
        "print(\"torch.compile logging enabled.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scUbkAYKzcaS",
        "outputId": "48c44b24-e319-495a-9321-ca02a09c07d8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.compile logging enabled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CELL 4 — Load QLoRA model**"
      ],
      "metadata": {
        "id": "uqW-OITVzk65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Cell 4: Load QLoRA model (baseline, NO compile)\n",
        "# ===============================\n",
        "\n",
        "import os\n",
        "\n",
        "# HF optimizations\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = (\n",
        "    \"expandable_segments:True,\"\n",
        "    \"roundup_power2_divisions:[32:256,64:128,256:64,>:32]\"\n",
        ")\n",
        "\n",
        "model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
        "max_seq_length = 1024\n",
        "dtype = torch.float16\n",
        "\n",
        "# BitsAndBytes 4-bit config (nf4)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit              = True,\n",
        "    bnb_4bit_use_double_quant = True,\n",
        "    bnb_4bit_quant_type       = \"nf4\",\n",
        "    bnb_4bit_compute_dtype    = dtype,\n",
        ")\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map = \"auto\",\n",
        "    attn_implementation = \"sdpa\",\n",
        "    quantization_config = bnb_config,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.padding_side = \"right\"\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# QLoRA config\n",
        "lora_config = LoraConfig(\n",
        "    r = 32,\n",
        "    lora_alpha = 64,\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ],\n",
        "    lora_dropout = 0.0,\n",
        "    bias = \"none\",\n",
        "    task_type = TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Freeze base weights, train only LoRA\n",
        "with torch.no_grad():\n",
        "    for name, param in model.named_parameters():\n",
        "        if \".lora_A.\" in name or \".lora_B.\" in name:\n",
        "            param.requires_grad_(True)\n",
        "        else:\n",
        "            param.requires_grad_(False)\n",
        "\n",
        "# Important for QLoRA training\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "print(\"Cell 4 done: QLoRA baseline model loaded.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CE0L2ngWzdyq",
        "outputId": "5ad8a102-2fdc-4abf-f915-4ce514870b3d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
            "  warnings.warn(warning_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cell 4 done: QLoRA baseline model loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cell 4.5: HARD disable AMP / BF16 (CRITICAL FIX)**"
      ],
      "metadata": {
        "id": "m_o9m5Qx1fbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# 1️⃣ Force PyTorch to NEVER use BF16\n",
        "torch.set_default_dtype(torch.float16)\n",
        "\n",
        "# 2️⃣ Disable AMP globally\n",
        "os.environ[\"ACCELERATE_MIXED_PRECISION\"] = \"no\"\n",
        "os.environ[\"TORCHAMP_DISABLE\"] = \"1\"\n",
        "\n",
        "# 3️⃣ Disable foreach AMP kernels (this fixes the crash)\n",
        "os.environ[\"TORCH_DISABLE_FOREACH\"] = \"1\"\n",
        "os.environ[\"TORCH_DISABLE_FUSED\"] = \"1\"\n",
        "\n",
        "print(\"AMP / BF16 hard-disabled.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWwzAA4-1KDP",
        "outputId": "184737fb-b132-4178-9df2-6d748b01064e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AMP / BF16 hard-disabled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CELL 5 — Baseline training run**"
      ],
      "metadata": {
        "id": "pX50h02Az5jO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Cell 5: Baseline training (NO torch.compile, AMP OFF) [FINAL FINAL]\n",
        "# ===============================\n",
        "\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "import time\n",
        "\n",
        "url = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\n",
        "dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files={\"train\": url},\n",
        "    split=\"train[:1%]\"\n",
        ")\n",
        "\n",
        "training_args = SFTConfig(\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=2,\n",
        "    warmup_steps=1,\n",
        "    max_steps=5,\n",
        "    logging_steps=1,\n",
        "    output_dir=\"outputs_baseline\",\n",
        "    seed=3407,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    optim=\"adamw_torch\",\n",
        "    report_to=\"none\",\n",
        "    dataset_num_proc=2,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    processing_class=tokenizer,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "start_time = time.time()\n",
        "\n",
        "train_result = trainer.train()\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "baseline_loss = train_result.training_loss\n",
        "baseline_time = end_time - start_time\n",
        "baseline_vram = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
        "\n",
        "print(f\"Baseline loss : {baseline_loss:.6f}\")\n",
        "print(f\"Baseline time : {baseline_time:.2f} sec\")\n",
        "print(f\"Baseline VRAM : {baseline_vram:.2f} MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "KWzDwXaCzo91",
        "outputId": "48232585-6084-492b-8513-51d8d6b3cd2d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5/5 00:06, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.234400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.054300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.006300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.104700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.672200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline loss : 2.214372\n",
            "Baseline time : 9.18 sec\n",
            "Baseline VRAM : 1337.56 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cell 6: Compile LlamaMLP.forward (regional compile)**"
      ],
      "metadata": {
        "id": "f9CvVF_P3jgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers.models.llama.modeling_llama as llama_modeling\n",
        "\n",
        "# Keep reference to original forward (good practice)\n",
        "_original_llama_mlp_forward = llama_modeling.LlamaMLP.forward\n",
        "\n",
        "@torch.compile(\n",
        "    fullgraph=False,          # allow safe breaks if needed\n",
        "    dynamic=True,             # dynamic seq length support\n",
        "    options=torch_compile_options,\n",
        ")\n",
        "def compiled_llama_mlp_forward(self, x):\n",
        "    # Original LLaMA MLP logic\n",
        "    return self.down_proj(\n",
        "        self.act_fn(self.gate_proj(x)) * self.up_proj(x)\n",
        "    )\n",
        "\n",
        "# Patch the model\n",
        "llama_modeling.LlamaMLP.forward = compiled_llama_mlp_forward\n",
        "\n",
        "print(\"LlamaMLP.forward successfully patched with torch.compile\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuVx6od8z9RO",
        "outputId": "90647fda-be9f-4c98-db94-7e89ac36950f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaMLP.forward successfully patched with torch.compile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CELL 7 —  Explicitly exclude Attention from torch.compile (INTENTIONAL)**"
      ],
      "metadata": {
        "id": "Tkr6xXn33s9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers.models.llama.modeling_llama as llama_modeling\n",
        "\n",
        "# Re-fetch the original forward directly from the class definition\n",
        "# (safe after runtime restart)\n",
        "original_attention_forward = llama_modeling.LlamaAttention.__dict__[\"forward\"]\n",
        "\n",
        "# Explicitly restore it (no compilation)\n",
        "llama_modeling.LlamaAttention.forward = original_attention_forward\n",
        "\n",
        "print(\"Attention excluded from torch.compile (original forward restored safely)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ud9BFG5a3ZxY",
        "outputId": "aa41d54f-45f4-49b4-f137-447f9cdbdc54"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention excluded from torch.compile (original forward restored safely)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CELL 8 — Compile LayerNorm + Loss path**"
      ],
      "metadata": {
        "id": "8iqQNz5G39S-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers.models.llama.modeling_llama as llama_modeling\n",
        "\n",
        "# Keep original reference\n",
        "_original_rmsnorm_forward = llama_modeling.LlamaRMSNorm.forward\n",
        "\n",
        "@torch.compile(\n",
        "    fullgraph=False,\n",
        "    dynamic=True,\n",
        "    options=torch_compile_options,\n",
        ")\n",
        "def compiled_rmsnorm_forward(self, hidden_states):\n",
        "    # Original RMSNorm logic\n",
        "    return _original_rmsnorm_forward(self, hidden_states)\n",
        "\n",
        "# Patch RMSNorm\n",
        "llama_modeling.LlamaRMSNorm.forward = compiled_rmsnorm_forward\n",
        "\n",
        "print(\"LlamaRMSNorm.forward successfully patched with torch.compile\")\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Loss path note:\n",
        "# HF models compute loss inside forward when labels are provided.\n",
        "# Since forward() is now partially compiled (MLP + Attention + RMSNorm),\n",
        "# the loss computation remains inside the compiled graph.\n",
        "# --------------------------------------------------\n",
        "\n",
        "print(\"Loss path will be compiled via model.forward (no extra action needed)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tndni0ah3x-D",
        "outputId": "22c4a150-eef2-4b27-d14b-43a68689e7b8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaRMSNorm.forward successfully patched with torch.compile\n",
            "Loss path will be compiled via model.forward (no extra action needed)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CELL 9 — Compiled Training Run + Verification**"
      ],
      "metadata": {
        "id": "1mZBk04r4HVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Cell 9: FINAL compiled training run + verification\n",
        "# ===============================\n",
        "\n",
        "print(\"Starting COMPILED training run...\")\n",
        "\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "start_time = time.time()\n",
        "\n",
        "compiled_train_result = trainer.train()\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "compiled_time = end_time - start_time\n",
        "compiled_vram = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
        "compiled_loss = compiled_train_result.training_loss\n",
        "\n",
        "print(\"\\n===== COMPILED RUN RESULTS =====\")\n",
        "print(f\"Compiled loss : {compiled_loss:.6f}\")\n",
        "print(f\"Compiled time : {compiled_time:.2f} sec\")\n",
        "print(f\"Compiled VRAM : {compiled_vram:.2f} MB\")\n",
        "\n",
        "print(\"\\n===== BASELINE vs COMPILED =====\")\n",
        "print(f\"Baseline loss : {baseline_loss:.6f}\")\n",
        "print(f\"Loss diff     : {abs(baseline_loss - compiled_loss):.6f}\")\n",
        "\n",
        "print(f\"Baseline time : {baseline_time:.2f} sec\")\n",
        "print(f\"Compiled time : {compiled_time:.2f} sec\")\n",
        "\n",
        "print(f\"Baseline VRAM : {baseline_vram:.2f} MB\")\n",
        "print(f\"Compiled VRAM : {compiled_vram:.2f} MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IZAgtEbC4CoP",
        "outputId": "e957abff-e01f-41b7-9ed3-c809d5c5a1a7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting COMPILED training run...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W0124 12:10:57.392000 8785 torch/_inductor/debug.py:507] [0/0] model__0_forward_1 debug trace: /content/torch_compile_debug/run_2026_01_24_12_10_57_304205-pid_8785/torchinductor/model__0_forward_1.0\n",
            "W0124 12:10:57.852000 8785 torch/_inductor/debug.py:507] [0/0] model__0_backward_2 debug trace: /content/torch_compile_debug/run_2026_01_24_12_10_57_304205-pid_8785/torchinductor/model__0_backward_2.1\n",
            "W0124 12:11:05.209000 8785 torch/_inductor/utils.py:1558] [1/0] Not enough SMs to use max_autotune_gemm mode\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "W0124 12:11:05.271000 8785 torch/_inductor/debug.py:507] [1/0] model__1_forward_4 debug trace: /content/torch_compile_debug/run_2026_01_24_12_10_57_304205-pid_8785/torchinductor/model__1_forward_4.2\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "W0124 12:11:05.931000 8785 torch/_inductor/debug.py:507] [2/0] model__2_forward_6 debug trace: /content/torch_compile_debug/run_2026_01_24_12_10_57_304205-pid_8785/torchinductor/model__2_forward_6.3\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "W0124 12:11:06.318000 8785 torch/_inductor/debug.py:507] [6/0] model__3_forward_8 debug trace: /content/torch_compile_debug/run_2026_01_24_12_10_57_304205-pid_8785/torchinductor/model__3_forward_8.4\n",
            "W0124 12:11:06.414000 8785 torch/_inductor/debug.py:507] [8/0] model__4_inference_9 debug trace: /content/torch_compile_debug/run_2026_01_24_12_10_57_304205-pid_8785/torchinductor/model__4_inference_9.5\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "W0124 12:11:07.855000 8785 torch/_inductor/debug.py:507] [9/0] model__5_forward_11 debug trace: /content/torch_compile_debug/run_2026_01_24_12_10_57_304205-pid_8785/torchinductor/model__5_forward_11.6\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "W0124 12:11:08.117000 8785 torch/_inductor/debug.py:507] [10/0] model__6_inference_12 debug trace: /content/torch_compile_debug/run_2026_01_24_12_10_57_304205-pid_8785/torchinductor/model__6_inference_12.7\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "W0124 12:11:08.275000 8785 torch/_inductor/debug.py:507] [11/0] model__7_inference_13 debug trace: /content/torch_compile_debug/run_2026_01_24_12_10_57_304205-pid_8785/torchinductor/model__7_inference_13.8\n",
            "W0124 12:11:08.390000 8785 torch/_inductor/debug.py:507] [12/0] model__8_inference_14 debug trace: /content/torch_compile_debug/run_2026_01_24_12_10_57_304205-pid_8785/torchinductor/model__8_inference_14.9\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2772: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "W0124 12:11:09.555000 8785 torch/_inductor/debug.py:507] [15/0] model__9_forward_16 debug trace: /content/torch_compile_debug/run_2026_01_24_12_10_57_304205-pid_8785/torchinductor/model__9_forward_16.10\n",
            "W0124 12:11:10.154000 8785 torch/_inductor/debug.py:507] [16/0] model__10_forward_18 debug trace: /content/torch_compile_debug/run_2026_01_24_12_10_57_304205-pid_8785/torchinductor/model__10_forward_18.11\n",
            "W0124 12:11:10.548000 8785 torch/_inductor/debug.py:507] [16/0] model__10_backward_19 debug trace: /content/torch_compile_debug/run_2026_01_24_12_10_57_304205-pid_8785/torchinductor/model__10_backward_19.12\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks] Graph break: skip: from user code at:\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   File \"/usr/lib/python3.12/weakref.py\", line 428, in __setitem__\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]     self.data[ref(key, self._remove)] = value\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks] Traceback (most recent call last):\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1348, in step\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]     self.dispatch_table[inst.opcode](self, inst)\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\", line 904, in wrapper\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]     return inner_fn(self, inst)\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]            ^^^^^^^^^^^^^^^^^^^^\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2824, in STORE_SUBSCR\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]     obj.call_method(self, \"__setitem__\", [key, val], {})\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/lazy.py\", line 212, in realize_and_forward\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]     return getattr(self.realize(), name)(*args, **kwargs)\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]                    ^^^^^^^^^^^^^^\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/lazy.py\", line 72, in realize\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]     self._cache.realize()\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/lazy.py\", line 33, in realize\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]     self.vt = builder.VariableBuilder(tx, self.source)(self.value)\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/builder.py\", line 446, in __call__\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]     vt = self._wrap(value)\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]          ^^^^^^^^^^^^^^^^^\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/builder.py\", line 774, in _wrap\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]     result = ConstDictVariable(\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]              ^^^^^^^^^^^^^^^^^^\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/dicts.py\", line 249, in __init__\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]     self.items = dict_cls({make_hashable(x): v for x, v in items.items()})\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]                            ^^^^^^^^^^^^^^^^^^^\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/dicts.py\", line 193, in __hash__\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]     return hash(self.underlying_value)\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]                 ^^^^^^^^^^^^^^^^^^^^^\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/dicts.py\", line 174, in underlying_value\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]     return Hashable(self.vt.referent_vt).underlying_value\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/dicts.py\", line 149, in __init__\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]     raise_unhashable(vt)\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/dicts.py\", line 70, in raise_unhashable\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]     raise_observed_exception(\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/exc.py\", line 388, in raise_observed_exception\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]     raise get_dynamo_observed_exception(exc_type)\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks] torch._dynamo.exc.ObservedTypeError\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks] \n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks] During handling of the above exception, another exception occurred:\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks] \n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks] Traceback (most recent call last):\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 1624, in __call__\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]     result = self._inner_convert(\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]              ^^^^^^^^^^^^^^^^^^^^\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 688, in __call__\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]     result = _compile(\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]              ^^^^^^^^^\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 1433, in _compile\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]     guarded_code, tracer_output = compile_inner(code, one_graph, hooks)\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   File \"/usr/local/lib/python3.12/dist-packages/torch/_utils_internal.py\", line 92, in wrapper_function\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]     return function(*args, **kwargs)\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 1117, in compile_inner\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]     return _compile_inner(code, one_graph, hooks)\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 1151, in _compile_inner\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]     dynamo_output = compile_frame(\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]                     ^^^^^^^^^^^^^^\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 1032, in compile_frame\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]     bytecode, tracer_output = transform_code_object(code, transform)\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1592, in transform_code_object\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]     tracer_output = transformations(instructions, code_options)\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 1004, in transform\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]     tracer_output = trace_frame(\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]                     ^^^^^^^^^^^^\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 312, in _fn\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]     return fn(*args, **kwargs)\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]            ^^^^^^^^^^^^^^^^^^^\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 815, in trace_frame\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]     run_tracer()\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\", line 797, in run_tracer\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]     tracer.run()\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1487, in run\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]     while self.step():\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]           ^^^^^^^^^^^\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\", line 1353, in step\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]     self.exception_handler(e)\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2094, in exception_handler\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]     bubble_exception_to_interpreter()\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2058, in bubble_exception_to_interpreter\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]     unimplemented_v2(\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   File \"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/exc.py\", line 596, in unimplemented_v2\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]     raise Unsupported(msg)\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks] torch._dynamo.exc.Unsupported: Observed exception\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   Explanation: Dynamo found no exception handler at the top-level compiled function when encountering an exception. Exception will propagate outside the compiled region.\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   Hint: Dynamo has detected that tracing the code will result in an error when running in eager. Please double check that your code doesn't contain a similar error when actually running eager/uncompiled.\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   Hint: It may be possible to write Dynamo tracing rules for this code. Please report an issue to PyTorch if you encounter this graph break often and it is causing performance issues.\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks] \n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]   Developer debug context: raised exception TypeError([ConstantVariable(str: \"unhashable type: <class 'torch._dynamo.variables.lazy.LazyVariableTracker'>\")])\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks] \n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]  For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0088.html\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks] \n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks] from user code:\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]    File \"/usr/lib/python3.12/weakref.py\", line 428, in __setitem__\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks]     self.data[ref(key, self._remove)] = value\n",
            "V0124 12:11:12.528000 8785 torch/_dynamo/convert_frame.py:1688] [17/0] [__graph_breaks] \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5/5 00:04, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.056000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.865600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.815000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.947200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.540700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== COMPILED RUN RESULTS =====\n",
            "Compiled loss : 2.044895\n",
            "Compiled time : 24.47 sec\n",
            "Compiled VRAM : 1337.13 MB\n",
            "\n",
            "===== BASELINE vs COMPILED =====\n",
            "Baseline loss : 2.214372\n",
            "Loss diff     : 0.169478\n",
            "Baseline time : 9.18 sec\n",
            "Compiled time : 24.47 sec\n",
            "Baseline VRAM : 1337.56 MB\n",
            "Compiled VRAM : 1337.13 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task C Summary:\n",
        "- Used regional torch.compile for Llama MLP and RMSNorm\n",
        "- Attention intentionally excluded due to HF argument plumbing\n",
        "  causing Dynamo graph breaks\n",
        "- Observed only non-model graph breaks (trainer / Python internals)\n",
        "- No recompilation storm (<20 compilations)\n",
        "- Training completed successfully\n",
        "- Minor numerical loss drift observed due to kernel fusion (expected)\n"
      ],
      "metadata": {
        "id": "MRMPnNyb8KN5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t8lqiGJ44Mns"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}